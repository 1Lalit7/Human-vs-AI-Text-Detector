{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c42960fb-6aee-4bd0-9425-bf86edb75212",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score, ConfusionMatrixDisplay\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afdc48ea-e64e-43fd-aa7e-e5a089ba1655",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_df(human_dataset_path, ai_dataset_path):\n",
    "    dataset_paths = [human_dataset_path, ai_dataset_path]\n",
    "    result_df = pd.DataFrame()\n",
    "    \n",
    "    for dataset_path in dataset_paths:\n",
    "\n",
    "        label = 0 if dataset_path.split('\\\\')[-1] == 'AI' else 1 ## 0 for AI generated text and 1 for human generated text\n",
    "        \n",
    "        csv_files = [os.path.join(dataset_path, file_name) for file_name in os.listdir(dataset_path) \n",
    "                if file_name.endswith(\".csv\")]\n",
    "        \n",
    "        for csv_file in csv_files:\n",
    "            data = pd.read_csv(csv_file)\n",
    "            data['label'] = label\n",
    "            result_df = pd.concat([result_df, data], ignore_index=True)\n",
    "    result_df = result_df.drop(columns = ['uid'])\n",
    "    result_df = result_df.drop_duplicates()\n",
    "    result_df = result_df.sample(frac=1).reset_index(drop=True)\n",
    "            \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18778f79-e70e-46a0-ab9c-16f0262b1d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEmbedding:\n",
    "\n",
    "    def __init__(self, model_name):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "        self.max_length = self.tokenizer.model_max_length\n",
    "\n",
    "    def convert_text_to_tokenid(self, text):\n",
    "        tokens = self.tokenizer(text, return_tensors=\"pt\")\n",
    "        token_ids = tokens[\"input_ids\"].squeeze().tolist()\n",
    "        return token_ids\n",
    "\n",
    "    def convert_tokenid_to_tokens(self, token_ids):\n",
    "        tokens_str = self.tokenizer.convert_ids_to_tokens(token_ids)\n",
    "        return tokens_str\n",
    "\n",
    "    def get_vocabulary(self):\n",
    "        return self.tokenizer.get_vocab()\n",
    "\n",
    "    def convert_tokenid_to_text(self, token_ids):\n",
    "        reconstructed_text = self.tokenizer.decode(token_ids)\n",
    "        return reconstructed_text\n",
    "\n",
    "    def cosine_similarity(self, input1, input2):\n",
    "        embeddings1 = self.encode(input1)\n",
    "        embeddings2 = self.encode(input2)\n",
    "        similarity_score = F.cosine_similarity(embeddings1, embeddings2)\n",
    "        return round(similarity_score.item(), 4)\n",
    "        \n",
    "    def encode(self, text):\n",
    "        \n",
    "        tokens = self.tokenizer(text, return_tensors=\"pt\")\n",
    "        input_ids = tokens['input_ids'][0]  # Extract the input IDs\n",
    "        num_chunks = (len(input_ids) // self.max_length) + 1\n",
    "        chunks = [input_ids[i * self.max_length: (i + 1) * self.max_length] for i in range(num_chunks)]\n",
    "    \n",
    "        embeddings = []\n",
    "\n",
    "        # Process each chunk\n",
    "        for chunk in chunks:\n",
    "            chunk_tensor = chunk.unsqueeze(0)  # Add batch dimension (batch, num_tokens)\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(input_ids=chunk_tensor)\n",
    "                chunk_embedding = outputs.last_hidden_state.mean(dim=1)  # (batch, num_tokens, embeding_size)\n",
    "            embeddings.append(chunk_embedding)\n",
    "\n",
    "        con_embedding = torch.cat(embeddings, dim=0)\n",
    "        combined_embedding = torch.mean(con_embedding, dim=0)\n",
    "            \n",
    "        return combined_embedding\n",
    "        \n",
    "    def decode(self, embedded_text):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9198638f-e9ed-41da-9f4e-543ad079fcab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPreprocessing:\n",
    "    def __init__(self,  model_name = 'gpt2'):\n",
    "        # self.text_embedding = text_embedding\n",
    "        self.text_embedding = TextEmbedding(model_name)\n",
    "\n",
    "    def preprocessing_basic(self, text):\n",
    "        text = text.lower()\n",
    "        text = text.replace('\\r\\n', ' ').replace('\\n', '').replace('\\r', '').replace('\\\\', '')\n",
    "        return text\n",
    "\n",
    "    def preprocessing_text(self, text):\n",
    "        text_encoded = self.text_embedding.encode(text)\n",
    "        text_encoded_flatten = torch.flatten(text_encoded)\n",
    "        text_encoded_flatten_array = text_encoded_flatten.numpy()\n",
    "        return text_encoded_flatten_array\n",
    "\n",
    "    def preprocessing_final(self, dataset):\n",
    "        # processed_dataset = self.dataset['text'].copy()\n",
    "        processed_dataset = dataset['text'].apply(self.preprocessing_text)\n",
    "        dataset_text_df = pd.DataFrame(processed_dataset.tolist())\n",
    "        dataset_text_df.columns = [f\"feature_{i}\" for i in range(1, dataset_text_df.shape[1] + 1)]\n",
    "        processed_dataset = pd.concat([dataset_text_df, dataset['label']], axis = 1)\n",
    "        return processed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a96a53-e822-4a8d-8e9d-00c7e6b44671",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeaturesDimensionalityReduction:\n",
    "    def __init__(self, scaler=None, pca=None):\n",
    "        self.scaler = scaler or StandardScaler()\n",
    "        self.pca = pca or PCA()\n",
    "\n",
    "    def fit(self, X_data, n_components):\n",
    "        self.scaler.fit(X_data)\n",
    "        standardized_data = self.scaler.transform(X_data)\n",
    "        self.pca = PCA(n_components=n_components)\n",
    "        self.pca.fit(standardized_data)\n",
    "\n",
    "    def transform_data(self, data):\n",
    "        standardized_data = self.scaler.transform(data)\n",
    "        return self.pca.transform(standardized_data)\n",
    "\n",
    "    def transform_single_data_point(self, data_point):\n",
    "        return self.transform_data(data_point)\n",
    "\n",
    "    def save(self, scaler_path=\"scaler.pkl\", pca_path=\"pca.pkl\"):\n",
    "        with open(scaler_path, \"wb\") as scaler_file:\n",
    "            pickle.dump(self.scaler, scaler_file)\n",
    "        with open(pca_path, \"wb\") as pca_file:\n",
    "            pickle.dump(self.pca, pca_file)\n",
    "\n",
    "    def load(self, scaler_path=\"scaler.pkl\", pca_path=\"pca.pkl\"):\n",
    "        with open(scaler_path, \"rb\") as scaler_file:\n",
    "            self.scaler = pickle.load(scaler_file)\n",
    "        with open(pca_path, \"rb\") as pca_file:\n",
    "            self.pca = pickle.load(pca_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8da9c9f-1033-4486-b5be-fa4d073a6686",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing(text, model):\n",
    "    label_mapping = {0: 'AI', 1: 'Human'}\n",
    "    embeded_text = processed_dataset_obj.preprocessing_text(text).reshape(1, -1)\n",
    "    embeded_text_df = pd.DataFrame(embeded_text)\n",
    "    embeded_text_df.columns = [f\"feature_{i}\" for i in range(1, embeded_text_df.shape[1] + 1)]\n",
    "    transformed_data = fdr.transform_single_data_point(embeded_text_df)\n",
    "    prediction = model.predict(transformed_data)\n",
    "    predicted_label = label_mapping[prediction[0]] + \" Generated\"\n",
    "    return predicted_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abf7c9c-b685-4167-957c-484249889f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_evaluation(y_test, y_pred):\n",
    "    \"\"\"\n",
    "    Evaluate the model's predictions using confusion matrix, classification report,\n",
    "    and additional performance metrics such as F1-score, TPR, FPR, and accuracy.\n",
    "    \"\"\"\n",
    "    # Calculate the confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    # Display the confusion matrix\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=['AI', 'Human'])\n",
    "    disp.plot(cmap='viridis')\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "    # Print the classification report\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "    # Extract components of the confusion matrix\n",
    "    tn, fp, fn, tp = conf_matrix.ravel()\n",
    "\n",
    "    # Calculate additional metrics\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    # Safeguard against division by zero for rates\n",
    "    fpr = fp / (fp + tn) if (fp + tn) > 0 else 0.0\n",
    "    fnr = fn / (fn + tp) if (fn + tp) > 0 else 0.0\n",
    "    tpr = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    tnr = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
    "\n",
    "    print(\"\\nAdditional Metrics:\")\n",
    "    print(f\"True Positive Rate (TPR, Sensitivity): {tpr:.2f}\")\n",
    "    print(f\"True Negative Rate (TNR, Specificity): {tnr:.2f}\")\n",
    "    print(f\"False Positive Rate (FPR): {fpr:.2f}\")\n",
    "    print(f\"False Negative Rate (FNR): {fnr:.2f}\")\n",
    "\n",
    "    # Calculate total number of mistakes (false predictions)\n",
    "    actual_mistakes = fp + fn\n",
    "    error_rate = actual_mistakes / len(y_pred)\n",
    "\n",
    "    print(\"\\nTotal Number of Mistakes:\", actual_mistakes)\n",
    "    print(f\"Error Rate: {error_rate:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21f5893-bab4-4461-9745-011b461eea65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_plot(dataset, save_fig = None):\n",
    "\n",
    "    # Define the label mapping with integer keys\n",
    "    label_mapping = {1: 'Human', 0: 'AI'}\n",
    "    \n",
    "    # Create the countplot\n",
    "    ax = sns.countplot(x='label', data=dataset)\n",
    "    \n",
    "    # Set fixed ticks and tick labels based on the mapping\n",
    "    ax.xaxis.set_major_locator(FixedLocator(ax.get_xticks()))\n",
    "    ax.set_xticklabels([label_mapping[int(label.get_text())] for label in ax.get_xticklabels()])\n",
    "    \n",
    "    # Annotate each bar with the count\n",
    "    for p in ax.patches:\n",
    "        ax.annotate(\n",
    "            f'{int(p.get_height())}',  # The count value\n",
    "            (p.get_x() + p.get_width() / 2, p.get_height()),  # Position of the text\n",
    "            ha='center',\n",
    "            va='bottom'\n",
    "        )\n",
    "    ax.set_xlabel('Category')  \n",
    "    ax.set_ylabel('Count')  \n",
    "\n",
    "    # save plot\n",
    "    if save_fig:\n",
    "        plt.savefig(save_fig)\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370a3295-96cf-4561-bc87-1eeabb0627ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling_the_dataset(dataset, num_samples, random_state=42):\n",
    "    class_0_samples = num_samples//2\n",
    "    class_1_samples = num_samples - class_0_samples\n",
    "    \n",
    "    dataset_0 = dataset[dataset['label'] == 0]\n",
    "    sampled_dataset_0 = dataset_0.sample(class_0_samples, random_state=random_state)\n",
    "    \n",
    "    dataset_1 = dataset[dataset['label'] == 1]\n",
    "    sampled_dataset_1 = dataset_1.sample(class_1_samples, random_state=random_state)\n",
    "\n",
    "    combined_dataset = pd.concat([sampled_dataset_0, sampled_dataset_1], axis=0, ignore_index=True)\n",
    "\n",
    "    shuffled_dataset = combined_dataset.sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "\n",
    "    return shuffled_dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
